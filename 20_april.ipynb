{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f82f33e-4275-48da-a811-763f75c78955",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "Answer --> KNN is a simple and intutive ml algorithm used for both reggression and classification tasks.It falls under the categery of lazy-learning, meaning it does'nt build a model during training but store the entair dataset and makes predictions by finding the nearest neighbore in the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c147928-09e3-4cbb-99de-f83a432a7b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "deb80848-3207-4eac-9b57-9613463cf1d3",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Answer--> Most Probably we don't use any fix K value directly for our model rather we use it as hyperparameter, so the best K value get sellected autometicaly while traing of model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c095b3c5-58a2-4f51-9efa-6add3da83f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88f132ab-baa9-4813-8297-1d1446c948be",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "Answer--> The basic diffrence between both of them is the output of the KNN classifier is majority vote of k neighbors data points whether for reggessor problem it is the median or avrage of the K neighbor data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee20733-0df5-4c7e-862b-531b0e26086a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b93cda3-4951-4e96-b7a1-eee837d7bee8",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "Answer--> We can measure the performance of K-Nearest Neighbors (KNN) using the following common evaluation metrics:\n",
    "\n",
    "1. **Accuracy**: The proportion of correctly classified instances out of the total number of instances in the dataset.\n",
    "\n",
    "2. **Precision**: The ratio of true positive predictions (correctly predicted positive instances) to the total predicted positive instances. It measures the model's ability to avoid false positive errors.\n",
    "\n",
    "3. **Recall (Sensitivity)**: The ratio of true positive predictions to the total actual positive instances. It measures the model's ability to identify all positive instances.\n",
    "\n",
    "4. **F1-Score**: The harmonic mean of precision and recall, providing a balance between precision and recall.\n",
    "\n",
    "5. **Mean Absolute Error (MAE)**: A metric for regression tasks, measuring the average absolute difference between predicted and actual values.\n",
    "\n",
    "6. **Mean Squared Error (MSE)**: Another regression metric, measuring the average squared difference between predicted and actual values.\n",
    "\n",
    "7. **Confusion Matrix**: A table that provides a detailed breakdown of true positive, true negative, false positive, and false negative predictions.\n",
    "\n",
    "8. **ROC Curve and AUC**: For binary classification, the Receiver Operating Characteristic (ROC) curve plots the trade-off between true positive rate and false positive rate, and the Area Under the Curve (AUC) quantifies the model's overall performance.\n",
    "\n",
    "9. **Cross-Validation**: Using k-fold cross-validation to assess how well the model generalizes to unseen data by splitting the dataset into training and testing subsets multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d4512-04a4-422a-802f-8537fdc59988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1cbb7bf-bc59-4c69-920d-cece0c5e0013",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "Answer-->Key challenges and implications of the curse of dimensionality in KNN include: \n",
    "\n",
    "1. **Increased Computational Complexity** : As the number of dimensions increases, the computational cost of calculating distances between data points grows exponentially. This can make KNN impractical for high-dimensional datasets because the algorithm becomes extremely slow.\n",
    "\n",
    "2. **Data Sparsity**: In high-dimensional spaces, data points tend to become sparse, meaning that most points are far away from each other. This can lead to issues when selecting nearest neighbors because it becomes more likely that two points are equidistant from the target point.\n",
    "\n",
    "3. **Overfitting**: KNN can suffer from overfitting in high-dimensional spaces, as it may start to capture noise in the data rather than meaningful patterns.\n",
    "\n",
    "4. **Diminished Discriminative Power**: In high-dimensional spaces, the distinction between \"nearest\" and \"distant\" neighbors becomes less meaningful, making KNN less effective at finding relevant neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b50b9-f402-4a37-9aa5-0fb395933d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d90b7a0-1cd4-45bf-850a-aa14f710f896",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Answer--> Use KNN imputation, which is a specific application of the KNN algorithm to impute missing values. Instead of predicting class labels or numerical values, KNN is used to predict missing feature values by considering the values of the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a63b1-475e-4ac9-8830-feaacf68acc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cc44ef7-ade6-48d9-8b2b-8fadf47d7456",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "Answer--> K-Nearest Neighbors (KNN) can be used as both a classifier and a regressor, and the choice between them depends on the nature of the problem you are trying to solve. Here's a comparison of the two and guidance on when each is more suitable:\n",
    "\n",
    "KNN Classifier:\n",
    "\n",
    "- Problem Type: KNN classification is used for solving problems where the target variable or outcome is categorical or belongs to a finite set of classes or labels.\n",
    "- Output: It predicts the class label or category that a new data point belongs to based on the majority class among its k nearest neighbors.\n",
    "- Evaluation Metrics: Classification evaluation metrics, such as accuracy, precision, recall, F1-score, and ROC-AUC, are used to assess the performance.\n",
    "- Examples: Image classification, spam email detection, sentiment analysis, and disease diagnosis (e.g., classifying diseases as \"malignant\" or \"benign\").\n",
    "\n",
    "KNN Regressor:\n",
    "\n",
    "- Problem Type: KNN regression is used for problems where the target variable is continuous and numerical, rather than categorical.\n",
    "- Output: It predicts a numerical value based on the average (or another aggregation method) of the target values of its k nearest neighbors.\n",
    "- Evaluation Metrics: Regression evaluation metrics, such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared, are used to assess performance.\n",
    "- Examples: Predicting house prices, stock market prices, temperature forecasting, and demand forecasting (e.g., predicting sales quantities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e9f81c-bc4b-48a9-b0a1-3a518123fe21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1d28a7c-7db1-41b7-9cf7-a98eb09229aa",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "Answer-->The K-Nearest Neighbors (KNN) algorithm has its strengths and weaknesses in both classification and regression tasks. Understanding these can help you make informed decisions when choosing KNN or addressing its limitations:\n",
    "\n",
    "**Strengths of KNN**:\n",
    "\n",
    "1. **Simplicity**: KNN is easy to understand and implement, making it a straightforward choice for simple classification and regression tasks.\n",
    "\n",
    "2. **Non-parametric**: KNN is non-parametric, meaning it doesn't make strong assumptions about the underlying data distribution. This flexibility allows it to work well with various types of data.\n",
    "\n",
    "3. **Versatility**: KNN can handle both classification and regression problems, providing a unified approach for different data analysis tasks.\n",
    "\n",
    "4. **Robustness to Outliers**: KNN can be robust to outliers since predictions are based on the majority of the nearest neighbors, reducing the influence of individual noisy data points.\n",
    "\n",
    "**Weaknesses of KNN**:\n",
    "\n",
    "1. **Computational Cost**: KNN can be computationally expensive, especially for large datasets or high-dimensional data. Calculating distances between data points becomes increasingly time-consuming as the dataset grows.\n",
    "\n",
    "2. **Sensitivity to Feature Scaling**: KNN is sensitive to the scale of features. Features with larger scales can dominate the distance calculations. It's essential to normalize or scale features to avoid bias.\n",
    "\n",
    "3. **Curse of Dimensionality**: In high-dimensional spaces, the effectiveness of KNN diminishes due to data sparsity, computational complexity, and difficulties in defining meaningful distance metrics.\n",
    "\n",
    "4. **Imbalanced Data**: KNN can struggle with imbalanced datasets where one class significantly outnumbers the others. It may classify most instances as the majority class.\n",
    "\n",
    "5. **Optimal K Selection**: Choosing the right value for K is a critical decision and can be challenging. An inappropriate choice can lead to underfitting or overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8345a410-e1c2-45e4-b3c0-0ec6457e18e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73eb6709-d7fa-4928-ba6f-87ad5d2ffa2b",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "Answer--> \n",
    "- **Euclidean Distance**: Measures the straight-line distance between two points. Sensitive to feature scaling and diagonal movement in space.\n",
    "\n",
    "- **Manhattan Distance**: Measures the distance by summing absolute differences along each dimension. Less sensitive to feature scaling, suitable for grid-like structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9c2a6-49e8-4baf-be8f-c1a13c11faed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f265437a-b7ea-42ca-a551-264857649c5f",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "Answer--> Feature scaling in KNN is essential to ensure that all features are on a consistent scale, allowing the algorithm to make distance-based comparisons effectively. Proper scaling can enhance KNN's accuracy and robustness, especially when dealing with data that has features with different units or magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d708a-611a-49da-ae5f-67bef36dd308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
