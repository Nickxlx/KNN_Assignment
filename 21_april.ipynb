{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a83cfa3a-9b87-49a7-af97-9782996f52f2",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "\n",
    "Answer--> The main difference is that the Euclidean distance takes into account the diagonal (shortest) distance between two points, whereas the Manhattan distance only considers horizontal and vertical movements.\n",
    "\n",
    "1. **Euclidean Distance**: This is the most common distance metric used in KNN. It calculates the straight-line (as-the-crow-flies) distance between two points in a Euclidean space. In a two-dimensional space, the Euclidean distance between points A(x1, y1) and B(x2, y2) is given by:\n",
    "\n",
    "    Euclidean Distance = âˆš((x2 - x1)^2 + (y2 - y1)^2)\n",
    "\n",
    "2. **Manhattan Distance**: Manhattan distance, as explained earlier, measures the distance by summing the absolute differences along each dimension. In a two-dimensional space, the Manhattan distance between points A(x1, y1) and B(x2, y2) is given by:\n",
    "\n",
    "    Manhattan Distance = |x2 - x1| + |y2 - y1|\n",
    "\n",
    "**Impact on KNN Performance**:\n",
    "\n",
    "1. **Sensitivity to Scale**: Euclidean distance is sensitive to the scale of the data because it involves squaring the differences between coordinates. This means that features with larger scales can dominate the distance calculation. In contrast, Manhattan distance is less affected by scale because it only considers absolute differences. If your dataset has features with very different scales, using Manhattan distance may lead to more balanced results.\n",
    "\n",
    "2. **Effect on Decision Boundaries**: The choice of distance metric can affect the shape of decision boundaries in KNN. Euclidean distance tends to create circular or spherical decision boundaries, which can be problematic when dealing with data that is not uniformly distributed. Manhattan distance, on the other hand, can create more square or grid-like decision boundaries, which may be more appropriate for certain types of data distributions.\n",
    "\n",
    "3. **Robustness to Outliers**: Manhattan distance can be more robust to outliers than Euclidean distance because outliers may have a significant impact on the squared differences used in Euclidean distance. Manhattan distance only considers absolute differences, which can reduce the influence of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b20bc7-5d06-4f65-a542-a05e40bce017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb11c28e-443e-4732-addf-99fcce74ce03",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
    "\n",
    "Answer-->Choosing the optimal value of k in a K-Nearest Neighbors (KNN) classifier or regressor is a critical step to ensure good model performance. The choice of k can significantly impact the model's ability to generalize to new data. Most Probably we don't use any fix K value directly for our model rather we use it as hyperparameter, so the best K value get sellected autometicaly while traing of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef0cf0b-29eb-46f3-9447-98c52e5e935e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6284ef4-e77e-4afe-82e7-102b59cc3fe1",
   "metadata": {},
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n",
    "\n",
    "Answer-->\n",
    "- **Euclidean Distance**: Measures the straight-line distance between two points. Sensitive to feature scaling and diagonal movement in space.\n",
    "\n",
    "- **Manhattan Distance**: Measures the distance by summing absolute differences along each dimension. Less sensitive to feature scaling, suitable for grid-like structures.\n",
    "\n",
    "Situations for Choosing One Distance Metric Over Another:\n",
    "\n",
    "Euclidean Distance:\n",
    "\n",
    "- When you have continuous, numeric data with similar feature scales.\n",
    "- When data points are distributed uniformly in all directions.\n",
    "- In high-dimensional spaces.\n",
    "\n",
    "Manhattan Distance:\n",
    "\n",
    "- When dealing with data that resembles a grid-like or lattice structure.\n",
    "- When you want a distance metric less sensitive to feature scale.\n",
    "- For mixed datasets with both categorical and numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83df83c-2de9-4da3-aec4-0b9ac40c5859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa07a6b8-f9af-4c2d-ba90-7eaf54a865e5",
   "metadata": {},
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
    "\n",
    "Answer--> Some of the comman huperparameter in KNN classifier anf Regressor are -\n",
    "\n",
    "**1 n_neighbors** :  This parammeter is use to set the value of K.\n",
    "- Impact: The most critical hyperparameter in KNN is the number of neighbors, 'k.' It determines how many nearby data points are considered when making predictions. Smaller values of 'k' can result in more flexible models, while larger values make the model smoother but can lead to underfitting.\n",
    "\n",
    "**2 weights** :  This parameter define the importance to nearest data point is equal or distance.\n",
    "- Impact: KNN can use different weighting schemes for neighbors. \"Uniform\" weighting treats all neighbors equally, while \"distance\" weighting gives more influence to closer neighbors.\n",
    "\n",
    "**3 algorithm** :  This parammeter is use to set best variant of KNN thes are ball_tree, kd_tree, brute. \n",
    "- Impact: KNN can use different algorithms to efficiently find nearest neighbors, such as brute-force search, KD-trees, or Ball trees. The choice can affect model training and prediction speed.\n",
    "\n",
    "**4 p** : This parammeter is use to set the distance formula which can be either 1 for Manhattan distance or 2 for Euclidean distance metric.\n",
    "- Impact: The choice of distance metric (e.g., Euclidean, Manhattan, Minkowski) influences how the algorithm measures the similarity between data points. It can affect the shape of decision boundaries and sensitivity to feature scale.\n",
    "\n",
    "To tune these hyperparameters, you can use techniques like cross-validation combined with grid search or randomized search. Cross-validation helps you evaluate model performance for different hyperparameter values, and grid/randomized search systematically explores a range of hyperparameter combinations to find the best configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984caab-6e26-4c50-af60-02d58a4939a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21ec8b4c-f885-4bc8-95c5-1aaba958f538",
   "metadata": {},
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?\n",
    "\n",
    "Answer-->  The size of the training set in K-Nearest Neighbors (KNN) can significantly affect model performance:\n",
    "\n",
    "- **Small Training Set**: More likely to overfit, capturing noise. Higher variance in performance.\n",
    "- **Large Training Set**: Less prone to overfitting, better generalization. Lower variance in performance.\n",
    "\n",
    "To optimize training set size:\n",
    "\n",
    "- Collect more data if possible.\n",
    "- Use data augmentation to generate more samples.\n",
    "- Apply cross-validation to assess performance with different training set sizes.\n",
    "- Consider bootstrapping, feature reduction, or transfer learning.\n",
    "- Explore ensemble methods, semi-supervised learning, active learning, and data pruning as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9cf7cb-be0c-4316-9a08-38365b9c09bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c19eab91-5ca8-46fc-bdf2-095a5bbe14d9",
   "metadata": {},
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?\n",
    "\n",
    "Answer--> some potential drawbacks of using KNN as a classifier or regressor are - \n",
    "\n",
    "- 1 Computational Complexity:\n",
    "\n",
    "Drawback: KNN can be computationally expensive, especially with large datasets, as it requires calculating distances to all training examples for each prediction.\n",
    "\n",
    "Solution: Consider using tree-based algorithms like KD-trees or Ball trees to speed up the nearest neighbor search. You can also reduce the dimensionality of your data or use approximate nearest neighbor techniques.\n",
    "\n",
    "- 2 Sensitivity to the Number of Neighbors (k):\n",
    "\n",
    "Drawback: The choice of 'k' can significantly impact model performance. Too small a 'k' may lead to overfitting, while too large a 'k' may result in underfitting.\n",
    "\n",
    "Solution: Use cross-validation or other hyperparameter tuning techniques to find the optimal 'k' value for your dataset.\n",
    "\n",
    "- 3 Sensitivity to Distance Metric:\n",
    "\n",
    "Drawback: KNN's performance can vary with different distance metrics, and choosing the wrong metric can lead to suboptimal results.\n",
    "\n",
    "Solution: Experiment with multiple distance metrics (e.g., Euclidean, Manhattan, Minkowski) to find the one that best suits your data. Use cross-validation to compare their performance.\n",
    "\n",
    "- 4 Imbalanced Data:\n",
    "\n",
    "Drawback: KNN can be biased towards the majority class in imbalanced datasets, leading to poor predictions for minority classes.\n",
    "\n",
    "Solution: Apply techniques like oversampling, undersampling, or using weighted distance metrics to address class imbalance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afbb079-42bd-44a7-9b6f-b83fc5616fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
